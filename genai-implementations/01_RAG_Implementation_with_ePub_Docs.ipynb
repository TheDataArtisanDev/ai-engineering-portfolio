{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a257124f",
   "metadata": {},
   "source": [
    "# RAG Implementation with ePub Documents\n",
    "\n",
    "This notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) system for ePub documents using FAISS and Langchain.\n",
    "\n",
    "## Features\n",
    "- ePub document loading and processing\n",
    "- Text chunking and embedding generation\n",
    "- Vector database storage with FAISS\n",
    "- Similarity-based document retrieval\n",
    "- AI-powered response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b52cc",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Installing required dependencies and importing necessary libraries for ePub processing, text embeddings, and vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2854999",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d1da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for ePub processing and document handling\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "import pypandoc\n",
    " \n",
    "# Install necessary libraries for document chunking, embeddings, and storage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    " \n",
    "# Import required libraries for Langchain OpenAI integration and text display\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from IPython.display import display, Markdown\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Pandoc if it is not already installed\n",
    "# pypandoc.download_pandoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eef48",
   "metadata": {},
   "source": [
    "## Natural Language Processing Setup\n",
    "\n",
    "Downloading NLTK resources for text tokenization and processing:\n",
    "- `punkt`: Sentence tokenizer\n",
    "- `punkt_tab`: Additional tokenization support  \n",
    "- `averaged_perceptron_tagger_eng`: Part-of-speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92aa816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required resources for text processing\n",
    "# nltk.download('punkt')  # Tokenizer for splitting sentences\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')  # POS tagger for sentence parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5a1bc",
   "metadata": {},
   "source": [
    "## ePub Document Loading\n",
    "\n",
    "Loading ePub documents using UnstructuredEPubLoader for content extraction and processing. Sample document download included for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0ea0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ePub file alice_wonderland.epub already exists.\n"
     ]
    }
   ],
   "source": [
    "# Download a sample ePub file for testing (Alice's Adventures in Wonderland from Project Gutenberg)\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# URL for a free ePub book from Project Gutenberg\n",
    "epub_url = \"https://www.gutenberg.org/ebooks/11.epub.noimages\"\n",
    "epub_filename = \"alice_wonderland.epub\"\n",
    "\n",
    "# Check if file already exists\n",
    "if not os.path.exists(epub_filename):\n",
    "    try:\n",
    "        print(f\"Downloading sample ePub file: {epub_filename}\")\n",
    "        urllib.request.urlretrieve(epub_url, epub_filename)\n",
    "        print(f\"Successfully downloaded {epub_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        print(\"Please manually download an ePub file or use a local file.\")\n",
    "else:\n",
    "    print(f\"ePub file {epub_filename} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3387d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ePub files: ['alice_wonderland.epub']\n",
      "\n",
      "Found 1 ePub file(s). You can use: alice_wonderland.epub\n"
     ]
    }
   ],
   "source": [
    "# Check for available ePub files in the current directory\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Look for ePub files\n",
    "epub_files = glob.glob(\"*.epub\")\n",
    "print(f\"Available ePub files: {epub_files}\")\n",
    "\n",
    "# If no ePub files found, provide instructions\n",
    "if not epub_files:\n",
    "    print(\"\\nNo ePub files found in the current directory.\")\n",
    "    print(\"Please either:\")\n",
    "    print(\"1. Download or place an ePub file in this directory\")\n",
    "    print(\"2. Update the file path in the loader to point to your ePub file\")\n",
    "    print(\"3. Use a sample ePub file from the internet (like Project Gutenberg)\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(epub_files)} ePub file(s). You can use: {epub_files[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b5a8c",
   "metadata": {},
   "source": [
    "## Document Content Extraction\n",
    "\n",
    "Extracting text content from ePub format and converting to structured document objects for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591b64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ePub file: alice_wonderland.epub\n",
      "Document loaded with 1 sections.\n",
      "Content preview (first 200 characters):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no r...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "import os\n",
    "\n",
    "# Upload ePub file to workspace or use sample download\n",
    "epub_file = \"alice_wonderland.epub\"\n",
    "if not os.path.exists(epub_file):\n",
    "    epub_file = \"three little pigs.epub\"  # fallback option\n",
    "    if not os.path.exists(epub_file):\n",
    "        print(\"ERROR: No ePub file found. Upload an ePub file to the workspace.\")\n",
    "        print(\"Or run the sample download cells above.\")\n",
    "    else:\n",
    "        print(f\"Using ePub file: {epub_file}\")\n",
    "else:\n",
    "    print(f\"Using ePub file: {epub_file}\")\n",
    "\n",
    "# Load and extract ePub content\n",
    "if os.path.exists(epub_file):\n",
    "    loader = UnstructuredEPubLoader(epub_file)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(f\"Document loaded with {len(docs)} sections.\")\n",
    "    \n",
    "    if docs:\n",
    "        print(f\"Content preview (first 200 characters):\")\n",
    "        print(docs[0].page_content[:200] + \"...\" if len(docs[0].page_content) > 200 else docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabc5ef",
   "metadata": {},
   "source": [
    "## Document Structure Analysis\n",
    "\n",
    "Analyzing the loaded document structure and content organization to verify successful extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4635788c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and wi\n"
     ]
    }
   ],
   "source": [
    "# Display the first section of the loaded document to verify data\n",
    "print(str(docs[0])[:200]) # Print the first chunk/section of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7a19b",
   "metadata": {},
   "source": [
    "# Text Processing and Vector Storage\n",
    "\n",
    "Converting documents into embeddings and storing in a vector database for efficient similarity search and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26f032",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "\n",
    "Splitting documents into manageable chunks using RecursiveCharacterTextSplitter with 300-character segments for optimal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32540daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 1053\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter with a chunk size of 300 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300)\n",
    " \n",
    "# Split the loaded documents into chunks\n",
    "chunks = text_splitter.split_documents(docs)\n",
    " \n",
    "# Print the number of chunks to verify successful splitting\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9551e7e",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "\n",
    "Creating vector embeddings using HuggingFace Sentence Transformers model (all-MiniLM-L6-v2) to capture semantic meaning of text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727ad661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaseen_banu\\AppData\\Local\\Temp\\ipykernel_28128\\1575397048.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\yaseen_banu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the Hugging Face embedding model for sentence embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    " \n",
    "# Print confirmation of embedding model loading\n",
    "print(\"Embedding model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a346b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd868b",
   "metadata": {},
   "source": [
    "## FAISS Vector Database\n",
    "\n",
    "Storing document embeddings in FAISS (Facebook AI Similarity Search) for efficient similarity search and clustering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75cec878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunks embedded and stored in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "# Embed the chunks as vectors and store them in a FAISS vector database\n",
    "db_faiss = FAISS.from_documents(chunks, embedding_model)\n",
    " \n",
    "# Print confirmation after FAISS database creation\n",
    "print(\"Document chunks embedded and stored in FAISS vector database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fde4e",
   "metadata": {},
   "source": [
    "# Document Retrieval System\n",
    "\n",
    "Implementing similarity-based document retrieval functionality for the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a995514",
   "metadata": {},
   "source": [
    "## Retrieval Function Implementation\n",
    "\n",
    "Creating a document retrieval function that performs similarity search on the FAISS database to find relevant content based on user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a89be978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant documents based on a user query\n",
    "def retrieve_docs(query, k):\n",
    "    # Perform similarity search on the FAISS database using the query\n",
    "    docs_faiss = db_faiss.similarity_search(query, k=k)\n",
    "    \n",
    "    # Return the most relevant document chunks\n",
    "    return docs_faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16062e",
   "metadata": {},
   "source": [
    "## Retrieval Testing\n",
    "\n",
    "Testing the retrieval system with sample queries to verify functionality and relevance of returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c939cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='CHAPTER I. Down the Rabbit-Hole' metadata={'source': 'alice_wonderland.epub'}\n"
     ]
    }
   ],
   "source": [
    "# Test the retrieval function with a specific query\n",
    "context = retrieve_docs(\"who is the antagonist of the story?\", 5)\n",
    " \n",
    "# Print the first retrieved chunk to verify correct retrieval\n",
    "print(context[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ddfa31",
   "metadata": {},
   "source": [
    "# AI Response Generation\n",
    "\n",
    "Integrating Large Language Model for generating contextual responses based on retrieved document content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4f28a",
   "metadata": {},
   "source": [
    "## Azure OpenAI Configuration\n",
    "\n",
    "Setting up Azure OpenAI client for GPT model integration with educational assistant persona configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32068884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env vars\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d617f83",
   "metadata": {},
   "source": [
    "## Response Generation Pipeline\n",
    "\n",
    "Complete RAG pipeline implementation combining document retrieval with AI-powered response generation for educational content delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00eebac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The antagonist of the story is the big bad wolf. An antagonist is a character who opposes the main character or creates conflict in the story. In this case, the wolf wants to eat the three little pigs, which makes him the one causing trouble for them. \n",
       "\n",
       "If you have any more questions about the story or need help with vocabulary, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the user query\n",
    "query = \"Who is the antagonist of the story?\"\n",
    "\n",
    "context = \"Once upon a time, there was a big bad wolf who wanted to eat the three little pigs. The wolf was very sneaky and tried to trick the pigs in many ways. He huffed and puffed and blew down their houses, but the pigs were clever and built strong houses to keep safe from the wolf.\"\n",
    "\n",
    "# Define the system prompt for the assistant's role\n",
    "system_message = f\"\"\"\n",
    "    You are a Kindergarten teacher helping the user learn through a story.\n",
    "    Include grammar explanations and clarifications for difficult vocabulary if applicable.\n",
    "    Correct the user's input when necessary.\n",
    "    Answer the query: {query} with the context: {context} provided.\n",
    "\"\"\"\n",
    "\n",
    "messages = [(\"system\", system_message), (\"human\", query)]\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Generate and display the response from the assistant\n",
    "response = llm.invoke(messages)  # Call the API with the messages\n",
    "display(Markdown(response.content))  # Display the response in markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26dd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
